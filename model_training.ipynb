{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get million song subset data song list\n",
    "Get metadata and join the data\n",
    "\n",
    "use artist similarity and artists to train the model on similarity\n",
    "\n",
    "use last.fm to get additional data on each song to augment this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import requests\n",
    "import time\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading million song subset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data (only loading song_id, metadata contains the rest)\n",
    "def read_song_features(file_path):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        song_id = f['metadata']['songs']['song_id'][0].decode('utf-8')\n",
    "        return {'song_id': song_id}\n",
    "\n",
    "\n",
    "# process all files in a directory into a df\n",
    "def process_all_files_to_dataframe(root_dir):\n",
    "    data = []\n",
    "    print(f\"Checking directory: {root_dir}\")\n",
    "\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        print(f\"Currently scanning {subdir} with {len(files)} files\")\n",
    "        for file in files:\n",
    "            if file.endswith('.h5'):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "                song_data = read_song_features(file_path)\n",
    "                data.append(song_data)\n",
    "\n",
    "    if not data:\n",
    "        print(\"No data to process.\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'data/MillionSongSubset'\n",
    "df = process_all_files_to_dataframe(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading million song subset metadata from sqlite db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata from sqlite\n",
    "def load_data_from_sqlite(db_path, table_name):\n",
    "    engine = create_engine(f'sqlite:///{db_path}')\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    return df\n",
    "\n",
    "# load metadata and merge with song data\n",
    "db_path3 = 'data/MillionSongSubsetMetadata/track_metadata.db'\n",
    "df3 = load_data_from_sqlite(db_path3, 'songs')\n",
    "df = df.merge(df3, on='song_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['track_id', 'artist_id', 'song_id', 'artist_mbid', 'track_7digitalid', 'shs_perf', 'shs_work']\n",
    "\n",
    "for column in columns_to_drop:\n",
    "    if column in df.columns:\n",
    "        df.drop(columns=[column], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading last.fm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(api_key, method, params):\n",
    "    base_url = \"http://ws.audioscrobbler.com/2.0/\"\n",
    "    params['api_key'] = api_key\n",
    "    params['method'] = method\n",
    "    params['format'] = 'json'\n",
    "    response = requests.get(base_url, params=params)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_artist_info(api_key, artist_name):\n",
    "    params = {'artist': artist_name}\n",
    "    return fetch_data(api_key, 'artist.getInfo', params)\n",
    "\n",
    "\n",
    "def get_track_info(api_key, artist_name, track_name):\n",
    "    params = {'artist': artist_name, 'track': track_name}\n",
    "    return fetch_data(api_key, 'track.getInfo', params)\n",
    "\n",
    "\n",
    "def batch_fetch_data(api_key, items, fetch_function, sleep_time=1):\n",
    "    results = []\n",
    "    for item in items:\n",
    "        result = fetch_function(api_key, *item)\n",
    "        results.append(result)\n",
    "        # time.sleep(sleep_time)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load LASTFM_API_KEY from .env\n",
    "import requests\n",
    "load_dotenv()\n",
    "api_key = os.getenv('LASTFM_API_KEY')\n",
    "\n",
    "\n",
    "def fetch_lastfm_data(api_key, artist_name, track_name):\n",
    "    base_url = \"http://ws.audioscrobbler.com/2.0/\"\n",
    "    params = {\n",
    "        'method': 'track.getInfo',\n",
    "        'api_key': api_key,\n",
    "        'artist': artist_name,\n",
    "        'track': track_name,\n",
    "        'format': 'json'\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200 and response.text.strip():\n",
    "        return response.json()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_lastfm_data(data):\n",
    "    if data and 'track' in data:\n",
    "        track = data['track']\n",
    "        return {\n",
    "            'listeners': track.get('listeners', '0'),\n",
    "            'playcount': track.get('playcount', '0'),\n",
    "            'tags': ', '.join(tag['name'] for tag in track.get('toptags', {}).get('tag', [])),\n",
    "        }\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 101/1000 [00:14<02:02,  7.36it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('LASTFM_API_KEY')\n",
    "subset_df = df.head(1000)\n",
    "\n",
    "tracks_skipped = 0\n",
    "\n",
    "\n",
    "def fetch_and_parse(row):\n",
    "    global tracks_skipped\n",
    "    data = fetch_lastfm_data(api_key, row['artist_name'], row['title'])\n",
    "    if data is None:\n",
    "        tracks_skipped += 1\n",
    "        return None\n",
    "    parsed_data = parse_lastfm_data(data)\n",
    "    if parsed_data is None:\n",
    "        tracks_skipped += 1\n",
    "    return parsed_data\n",
    "\n",
    "\n",
    "# Use progress_apply instead of apply\n",
    "subset_df['lastfm_data'] = subset_df.progress_apply(fetch_and_parse, axis=1)\n",
    "\n",
    "# Remove rows where lastfm_data is None\n",
    "subset_df = subset_df[subset_df['lastfm_data'].notna()]\n",
    "\n",
    "subset_df.reset_index(drop=True, inplace=True)\n",
    "track_details_df = pd.json_normalize(subset_df['lastfm_data'])\n",
    "mixed = pd.concat(\n",
    "    [subset_df.drop(columns=['lastfm_data']), track_details_df], axis=1)\n",
    "\n",
    "print(f\"Tracks skipped: {tracks_skipped}\")\n",
    "\n",
    "mixed.to_csv('data/music_data_small.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/music_data.csv')\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "\n",
    "# Encode categorical data\n",
    "label_encoders = {}\n",
    "unknown_label = 'unknown'  # Define an unknown label\n",
    "\n",
    "for column in ['artist_name', 'tags', 'title']:\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    # Get unique categories plus an 'unknown' category\n",
    "    unique_categories = df[column].unique().tolist()\n",
    "    # Add 'unknown' to the list of categories\n",
    "    unique_categories.append(unknown_label)\n",
    "\n",
    "    # Fit the LabelEncoder to these categories\n",
    "    le.fit(unique_categories)\n",
    "    df[column] = le.transform(df[column].astype(str))\n",
    "\n",
    "    # Store the encoder\n",
    "    label_encoders[column] = le\n",
    "\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = MinMaxScaler()\n",
    "df[['duration', 'listeners', 'playcount']] = scaler.fit_transform(\n",
    "    df[['duration', 'listeners', 'playcount']])\n",
    "\n",
    "# Split data into features and target\n",
    "X = df[['artist_name', 'tags', 'duration', 'listeners', 'playcount']]\n",
    "y = df['title']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongRecommender(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SongRecommender, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 128)  # Adjust input features if needed\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        # Output size = number of unique titles including 'unknown'\n",
    "        # Add 1 for the 'unknown' label\n",
    "        self.output = nn.Linear(128, len(y.unique()) + 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SongRecommender()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test):\n",
    "    train_loader = DataLoader(\n",
    "        list(zip(X_train.values.astype(float), y_train)), batch_size=50, shuffle=True)\n",
    "    test_loader = DataLoader(\n",
    "        list(zip(X_test.values.astype(float), y_test)), batch_size=50, shuffle=False)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(50):  # Number of epochs\n",
    "        train_loss = 0\n",
    "        for features, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(torch.tensor(features).float())\n",
    "            # Ensure labels are long type\n",
    "            loss = criterion(outputs, torch.tensor(labels).long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        validation_loss = 0\n",
    "        for features, labels in test_loader:\n",
    "            outputs = model(torch.tensor(features).float())\n",
    "            loss = criterion(outputs, torch.tensor(labels).long())\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {train_loss / len(train_loader)}, Validation Loss: {validation_loss / len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8592/2178768066.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = model(torch.tensor(features).float())\n",
      "/tmp/ipykernel_8592/2178768066.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = criterion(outputs, torch.tensor(labels).long())\n",
      "/tmp/ipykernel_8592/2178768066.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = model(torch.tensor(features).float())\n",
      "/tmp/ipykernel_8592/2178768066.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = criterion(outputs, torch.tensor(labels).long())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 16.780308064119314, Validation Loss: 8.522678647722516\n",
      "Epoch 2, Training Loss: 8.476378994223513, Validation Loss: 8.581566174825033\n",
      "Epoch 3, Training Loss: 8.462711946463878, Validation Loss: 8.638802800859724\n",
      "Epoch 4, Training Loss: 8.450367338863419, Validation Loss: 8.694792066301618\n",
      "Epoch 5, Training Loss: 8.439120492817443, Validation Loss: 8.749786604018439\n",
      "Epoch 6, Training Loss: 8.42882325914171, Validation Loss: 8.803766341436477\n",
      "Epoch 7, Training Loss: 8.41938112988884, Validation Loss: 8.85675434839158\n",
      "Epoch 8, Training Loss: 8.410704954170887, Validation Loss: 8.908784911746071\n",
      "Epoch 9, Training Loss: 8.402699576483833, Validation Loss: 8.959967658633278\n",
      "Epoch 10, Training Loss: 8.395319950433425, Validation Loss: 9.010271208626884\n",
      "Epoch 11, Training Loss: 8.388463820940183, Validation Loss: 9.059756823948451\n",
      "Epoch 12, Training Loss: 8.382111243259759, Validation Loss: 9.108405294872465\n",
      "Epoch 13, Training Loss: 8.376208894046737, Validation Loss: 9.156308582850865\n",
      "Epoch 14, Training Loss: 8.370698116443775, Validation Loss: 9.203451837812151\n",
      "Epoch 15, Training Loss: 8.365571799101653, Validation Loss: 9.249865804399763\n",
      "Epoch 16, Training Loss: 8.36077465245753, Validation Loss: 9.29563349769229\n",
      "Epoch 17, Training Loss: 8.356279267205132, Validation Loss: 9.340773718697685\n",
      "Epoch 18, Training Loss: 8.352082240728684, Validation Loss: 9.385225023542132\n",
      "Epoch 19, Training Loss: 8.348123244297357, Validation Loss: 9.429116067432222\n",
      "Epoch 20, Training Loss: 8.344407705613124, Validation Loss: 9.472454979306175\n",
      "Epoch 21, Training Loss: 8.340902422681268, Validation Loss: 9.515195437840053\n",
      "Epoch 22, Training Loss: 8.337617956561807, Validation Loss: 9.55745842343285\n",
      "Epoch 23, Training Loss: 8.334490834930797, Validation Loss: 9.599182537623815\n",
      "Epoch 24, Training Loss: 8.331546642162182, Validation Loss: 9.640476453871955\n",
      "Epoch 25, Training Loss: 8.328760264832297, Validation Loss: 9.681268510364351\n",
      "Epoch 26, Training Loss: 8.326126169275355, Validation Loss: 9.721652167184013\n",
      "Epoch 27, Training Loss: 8.323627907552837, Validation Loss: 9.761626243591309\n",
      "Epoch 28, Training Loss: 8.32125626081302, Validation Loss: 9.801218759445916\n",
      "Epoch 29, Training Loss: 8.318993580194167, Validation Loss: 9.840371813092913\n",
      "Epoch 30, Training Loss: 8.316843044610671, Validation Loss: 9.879224686395554\n",
      "Epoch 31, Training Loss: 8.314795552948375, Validation Loss: 9.917660259065174\n",
      "Epoch 32, Training Loss: 8.312847773234049, Validation Loss: 9.955856595720563\n",
      "Epoch 33, Training Loss: 8.31099596141297, Validation Loss: 9.993650708879743\n",
      "Epoch 34, Training Loss: 8.309215027608989, Validation Loss: 10.031209809439522\n",
      "Epoch 35, Training Loss: 8.307538833147214, Validation Loss: 10.068447930472237\n",
      "Epoch 36, Training Loss: 8.305911970727237, Validation Loss: 10.105395680382138\n",
      "Epoch 37, Training Loss: 8.304365475972494, Validation Loss: 10.14212771824428\n",
      "Epoch 38, Training Loss: 8.302881994365174, Validation Loss: 10.178618567330497\n",
      "Epoch 39, Training Loss: 8.301464080810547, Validation Loss: 10.214828718276252\n",
      "Epoch 40, Training Loss: 8.300105247968508, Validation Loss: 10.250786236354283\n",
      "Epoch 41, Training Loss: 8.298804883603696, Validation Loss: 10.286581993103027\n",
      "Epoch 42, Training Loss: 8.29754333731569, Validation Loss: 10.322115807306199\n",
      "Epoch 43, Training Loss: 8.2963564955158, Validation Loss: 10.357498759315128\n",
      "Epoch 44, Training Loss: 8.295203232470854, Validation Loss: 10.39269006819952\n",
      "Epoch 45, Training Loss: 8.294099678227932, Validation Loss: 10.427643775939941\n",
      "Epoch 46, Training Loss: 8.293049447330429, Validation Loss: 10.462472234453474\n",
      "Epoch 47, Training Loss: 8.29200033493984, Validation Loss: 10.497091974530901\n",
      "Epoch 48, Training Loss: 8.291021994602533, Validation Loss: 10.531586011250814\n",
      "Epoch 49, Training Loss: 8.29008913628849, Validation Loss: 10.565959340050107\n",
      "Epoch 50, Training Loss: 8.289165532147443, Validation Loss: 10.600136484418597\n"
     ]
    }
   ],
   "source": [
    "train_model(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = SongRecommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_songs(model, input_features):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            artist_index = label_encoders['artist_name'].transform(\n",
    "                [input_features['artist_name']])\n",
    "        except ValueError:\n",
    "            artist_index = label_encoders['artist_name'].transform(['unknown'])\n",
    "\n",
    "        try:\n",
    "            tags_index = label_encoders['tags'].transform(\n",
    "                [input_features['tags']])\n",
    "        except ValueError:\n",
    "            tags_index = label_encoders['tags'].transform(['unknown'])\n",
    "\n",
    "        # Create a DataFrame with feature names\n",
    "        scaled_features = pd.DataFrame(\n",
    "            [[input_features['duration'], input_features['listeners'],\n",
    "                input_features['playcount']]],\n",
    "            columns=['duration', 'listeners', 'playcount']\n",
    "        )\n",
    "        scaled_features = scaler.transform(scaled_features)[0]\n",
    "\n",
    "        features = torch.tensor(\n",
    "            [artist_index[0], tags_index[0], *scaled_features]).float().unsqueeze(0)\n",
    "        predictions = model(features)\n",
    "        top_5_values, top_5_indices = predictions.topk(5)\n",
    "        recommended_song_ids = top_5_indices.squeeze().tolist()\n",
    "        \n",
    "        return label_encoders['title'].inverse_transform(recommended_song_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def fetch_song_data(api_key, artist_name, track_name):\n",
    "    url = \"http://ws.audioscrobbler.com/2.0/\"\n",
    "    params = {\n",
    "        'method': 'track.getInfo',\n",
    "        'api_key': api_key,\n",
    "        'artist': artist_name,\n",
    "        'track': track_name,\n",
    "        'format': 'json'\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json() if response.status_code == 200 else {}\n",
    "\n",
    "\n",
    "def parse_song_data(song_data):\n",
    "    if song_data and 'track' in song_data:\n",
    "        track = song_data['track']\n",
    "        return {\n",
    "            'artist_name': track['artist']['name'],\n",
    "            'tags': ', '.join([tag['name'] for tag in track.get('toptags', {}).get('tag', [])]),\n",
    "            'duration': float(track.get('duration', 0)),\n",
    "            'listeners': int(track.get('listeners', 0)),\n",
    "            'playcount': int(track.get('playcount', 0)),\n",
    "            'album': track.get('album', {}).get('title', 'Unknown')\n",
    "        }\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Give It To Me', 'Pretty Smiles & Shattered Teeth',\n",
       "       'Nuoruusmuistoja', 'bereit', 'Chant For Eschaton 2000'],\n",
       "      dtype='<U97')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('LASTFM_API_KEY')\n",
    "\n",
    "artist_name = 'Lagy Gaga'\n",
    "track_name = 'Poker Face'\n",
    "\n",
    "# Fetch and parse song data\n",
    "song_data = fetch_song_data(api_key, artist_name, track_name)\n",
    "parsed_data = parse_song_data(song_data)\n",
    "\n",
    "# if the song is not found, or the tags column is empty, print a message\n",
    "if not parsed_data or not parsed_data['tags']:\n",
    "    print(\"Song not found or tags not available.\")\n",
    "\n",
    "\n",
    "recommend_songs(model, parsed_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
