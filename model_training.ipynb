{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get million song subset data song list\n",
    "Get metadata and join the data\n",
    "\n",
    "use artist similarity and artists to train the model on similarity\n",
    "\n",
    "use last.fm to get additional data on each song to augment this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import requests\n",
    "import time\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading million song subset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data (only loading song_id, metadata contains the rest)\n",
    "def read_song_features(file_path):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        song_id = f['metadata']['songs']['song_id'][0].decode('utf-8')\n",
    "        return {'song_id': song_id}\n",
    "\n",
    "\n",
    "# process all files in a directory into a df\n",
    "def process_all_files_to_dataframe(root_dir):\n",
    "    data = []\n",
    "    print(f\"Checking directory: {root_dir}\")\n",
    "\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        print(f\"Currently scanning {subdir} with {len(files)} files\")\n",
    "        for file in files:\n",
    "            if file.endswith('.h5'):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "                song_data = read_song_features(file_path)\n",
    "                data.append(song_data)\n",
    "\n",
    "    if not data:\n",
    "        print(\"No data to process.\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'data/MillionSongSubset'\n",
    "df = process_all_files_to_dataframe(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading million song subset metadata from sqlite db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata from sqlite\n",
    "def load_data_from_sqlite(db_path, table_name):\n",
    "    engine = create_engine(f'sqlite:///{db_path}')\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    return df\n",
    "\n",
    "# load metadata and merge with song data\n",
    "db_path3 = 'data/MillionSongSubsetMetadata/track_metadata.db'\n",
    "df3 = load_data_from_sqlite(db_path3, 'songs')\n",
    "df = df.merge(df3, on='song_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['track_id', 'artist_id', 'song_id', 'artist_mbid', 'track_7digitalid', 'shs_perf', 'shs_work']\n",
    "\n",
    "for column in columns_to_drop:\n",
    "    if column in df.columns:\n",
    "        df.drop(columns=[column], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading last.fm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(api_key, method, params):\n",
    "    base_url = \"http://ws.audioscrobbler.com/2.0/\"\n",
    "    params['api_key'] = api_key\n",
    "    params['method'] = method\n",
    "    params['format'] = 'json'\n",
    "    response = requests.get(base_url, params=params)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_artist_info(api_key, artist_name):\n",
    "    params = {'artist': artist_name}\n",
    "    return fetch_data(api_key, 'artist.getInfo', params)\n",
    "\n",
    "\n",
    "def get_track_info(api_key, artist_name, track_name):\n",
    "    params = {'artist': artist_name, 'track': track_name}\n",
    "    return fetch_data(api_key, 'track.getInfo', params)\n",
    "\n",
    "\n",
    "def batch_fetch_data(api_key, items, fetch_function, sleep_time=1):\n",
    "    results = []\n",
    "    for item in items:\n",
    "        result = fetch_function(api_key, *item)\n",
    "        results.append(result)\n",
    "        # time.sleep(sleep_time)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load LASTFM_API_KEY from .env\n",
    "import requests\n",
    "load_dotenv()\n",
    "api_key = os.getenv('LASTFM_API_KEY')\n",
    "\n",
    "\n",
    "def fetch_lastfm_data(api_key, artist_name, track_name):\n",
    "    base_url = \"http://ws.audioscrobbler.com/2.0/\"\n",
    "    params = {\n",
    "        'method': 'track.getInfo',\n",
    "        'api_key': api_key,\n",
    "        'artist': artist_name,\n",
    "        'track': track_name,\n",
    "        'format': 'json'\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200 and response.text.strip():\n",
    "        return response.json()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_lastfm_data(data):\n",
    "    if data and 'track' in data:\n",
    "        track = data['track']\n",
    "        return {\n",
    "            'listeners': track.get('listeners', '0'),\n",
    "            'playcount': track.get('playcount', '0'),\n",
    "            'tags': ', '.join(tag['name'] for tag in track.get('toptags', {}).get('tag', [])),\n",
    "        }\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 101/1000 [00:14<02:02,  7.36it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('LASTFM_API_KEY')\n",
    "subset_df = df.head(1000)\n",
    "\n",
    "tracks_skipped = 0\n",
    "\n",
    "\n",
    "def fetch_and_parse(row):\n",
    "    global tracks_skipped\n",
    "    data = fetch_lastfm_data(api_key, row['artist_name'], row['title'])\n",
    "    if data is None:\n",
    "        tracks_skipped += 1\n",
    "        return None\n",
    "    parsed_data = parse_lastfm_data(data)\n",
    "    if parsed_data is None:\n",
    "        tracks_skipped += 1\n",
    "    return parsed_data\n",
    "\n",
    "\n",
    "# Use progress_apply instead of apply\n",
    "subset_df['lastfm_data'] = subset_df.progress_apply(fetch_and_parse, axis=1)\n",
    "\n",
    "# Remove rows where lastfm_data is None\n",
    "subset_df = subset_df[subset_df['lastfm_data'].notna()]\n",
    "\n",
    "subset_df.reset_index(drop=True, inplace=True)\n",
    "track_details_df = pd.json_normalize(subset_df['lastfm_data'])\n",
    "mixed = pd.concat(\n",
    "    [subset_df.drop(columns=['lastfm_data']), track_details_df], axis=1)\n",
    "\n",
    "print(f\"Tracks skipped: {tracks_skipped}\")\n",
    "\n",
    "mixed.to_csv('data/music_data_small.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/music_data.csv')\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "\n",
    "# Encode categorical data\n",
    "label_encoders = {}\n",
    "unknown_label = 'unknown'  # Define an unknown label\n",
    "\n",
    "for column in ['artist_name', 'tags', 'title']:\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    # Get unique categories plus an 'unknown' category\n",
    "    unique_categories = df[column].unique().tolist()\n",
    "    # Add 'unknown' to the list of categories\n",
    "    unique_categories.append(unknown_label)\n",
    "\n",
    "    # Fit the LabelEncoder to these categories\n",
    "    le.fit(unique_categories)\n",
    "    df[column] = le.transform(df[column].astype(str))\n",
    "\n",
    "    # Store the encoder\n",
    "    label_encoders[column] = le\n",
    "\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = MinMaxScaler()\n",
    "df[['duration', 'listeners', 'playcount']] = scaler.fit_transform(\n",
    "    df[['duration', 'listeners', 'playcount']])\n",
    "\n",
    "# Split data into features and target\n",
    "X = df[['artist_name', 'tags', 'duration', 'listeners', 'playcount']]\n",
    "y = df['title']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongRecommender(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SongRecommender, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 128)  # Adjust input features if needed\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        # Output size = number of unique titles including 'unknown'\n",
    "        # Add 1 for the 'unknown' label\n",
    "        self.output = nn.Linear(128, len(y.unique()) + 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SongRecommender()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test):\n",
    "    train_loader = DataLoader(\n",
    "        list(zip(X_train.values.astype(float), y_train)), batch_size=50, shuffle=True)\n",
    "    test_loader = DataLoader(\n",
    "        list(zip(X_test.values.astype(float), y_test)), batch_size=50, shuffle=False)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(10):  # Number of epochs\n",
    "        train_loss = 0\n",
    "        for features, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(torch.tensor(features).float())\n",
    "            # Ensure labels are long type\n",
    "            loss = criterion(outputs, torch.tensor(labels).long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        validation_loss = 0\n",
    "        for features, labels in test_loader:\n",
    "            outputs = model(torch.tensor(features).float())\n",
    "            loss = criterion(outputs, torch.tensor(labels).long())\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {train_loss / len(train_loader)}, Validation Loss: {validation_loss / len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8592/3011880799.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = model(torch.tensor(features).float())\n",
      "/tmp/ipykernel_8592/3011880799.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = criterion(outputs, torch.tensor(labels).long())\n",
      "/tmp/ipykernel_8592/3011880799.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = model(torch.tensor(features).float())\n",
      "/tmp/ipykernel_8592/3011880799.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = criterion(outputs, torch.tensor(labels).long())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 15.853079501493477, Validation Loss: 8.523108754839216\n",
      "Epoch 2, Training Loss: 8.47606591825132, Validation Loss: 8.582034428914389\n",
      "Epoch 3, Training Loss: 8.462377866109213, Validation Loss: 8.63951537722633\n",
      "Epoch 4, Training Loss: 8.449959908002688, Validation Loss: 8.69593588511149\n",
      "Epoch 5, Training Loss: 8.438638180862238, Validation Loss: 8.751291093372163\n",
      "Epoch 6, Training Loss: 8.428259943738396, Validation Loss: 8.805644398643857\n",
      "Epoch 7, Training Loss: 8.418747219038599, Validation Loss: 8.859075591677712\n",
      "Epoch 8, Training Loss: 8.40998915684076, Validation Loss: 8.91154280162993\n",
      "Epoch 9, Training Loss: 8.401934929835944, Validation Loss: 8.96310533796038\n",
      "Epoch 10, Training Loss: 8.394473370210624, Validation Loss: 9.013808522905622\n"
     ]
    }
   ],
   "source": [
    "train_model(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_songs(model, input_features):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            artist_index = label_encoders['artist_name'].transform(\n",
    "                [input_features['artist_name']])\n",
    "        except ValueError:\n",
    "            artist_index = label_encoders['artist_name'].transform(['unknown'])\n",
    "\n",
    "        try:\n",
    "            tags_index = label_encoders['tags'].transform(\n",
    "                [input_features['tags']])\n",
    "        except ValueError:\n",
    "            tags_index = label_encoders['tags'].transform(['unknown'])\n",
    "\n",
    "        # Create a DataFrame with feature names\n",
    "        scaled_features = pd.DataFrame(\n",
    "            [[input_features['duration'], input_features['listeners'],\n",
    "                input_features['playcount']]],\n",
    "            columns=['duration', 'listeners', 'playcount']\n",
    "        )\n",
    "        scaled_features = scaler.transform(scaled_features)[0]\n",
    "\n",
    "        features = torch.tensor(\n",
    "            [artist_index[0], tags_index[0], *scaled_features]).float().unsqueeze(0)\n",
    "        predictions = model(features)\n",
    "        top_5_values, top_5_indices = predictions.topk(5)\n",
    "        recommended_song_ids = top_5_indices.squeeze().tolist()\n",
    "        \n",
    "        return label_encoders['title'].inverse_transform(recommended_song_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def fetch_song_data(api_key, artist_name, track_name):\n",
    "    url = \"http://ws.audioscrobbler.com/2.0/\"\n",
    "    params = {\n",
    "        'method': 'track.getInfo',\n",
    "        'api_key': api_key,\n",
    "        'artist': artist_name,\n",
    "        'track': track_name,\n",
    "        'format': 'json'\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json() if response.status_code == 200 else {}\n",
    "\n",
    "\n",
    "def parse_song_data(song_data):\n",
    "    if song_data and 'track' in song_data:\n",
    "        track = song_data['track']\n",
    "        return {\n",
    "            'artist_name': track['artist']['name'],\n",
    "            'tags': ', '.join([tag['name'] for tag in track.get('toptags', {}).get('tag', [])]),\n",
    "            'duration': float(track.get('duration', 0)),\n",
    "            'listeners': int(track.get('listeners', 0)),\n",
    "            'playcount': int(track.get('playcount', 0)),\n",
    "            'album': track.get('album', {}).get('title', 'Unknown')\n",
    "        }\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Smile', 'Hey Joe', 'Intro', 'Macarena',\n",
       "       'Skit #2 (Kanye West/Late Registration)'], dtype='<U97')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('LASTFM_API_KEY')\n",
    "\n",
    "artist_name = 'Lagy Gaga'\n",
    "track_name = 'Poker Face'\n",
    "\n",
    "# Fetch and parse song data\n",
    "song_data = fetch_song_data(api_key, artist_name, track_name)\n",
    "parsed_data = parse_song_data(song_data)\n",
    "\n",
    "# if the song is not found, or the tags column is empty, print a message\n",
    "if not parsed_data or not parsed_data['tags']:\n",
    "    print(\"Song not found or tags not available.\")\n",
    "\n",
    "\n",
    "recommend_songs(model, parsed_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
